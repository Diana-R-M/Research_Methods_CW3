
@article{zhang_change_2024,
	title = {Change detection with incorporating multi-constraints and loss weights},
	volume = {133},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095219762400321X},
	doi = {10.1016/j.engappai.2024.108163},
	abstract = {The past decades have seen significant progress in change detection for remote sensing images, particularly in urban change analysis and land management. The current mainstream change detection models mainly adopt the network structures of the Combination of Siamese networks and UNet. However, as the previous research mentioned, there are still several challenges to be tackled. On the one hand, A higher number of false positive samples than is the case in false negative samples, in other words, false pixel always occurs in the background region. On the other hand, speckle noise is also a serious problem to be resolved. To address these limitations, our work proposes a novel method, named the Change Detection model with incorporating Multi-Constraints and loss weights (CDMC). We introduce the similarity constraint and the boundary complementation information constraint into SUNet for the first time and design the adaptive dual focus loss module and adaptive weighted loss module. Similarity constraint is composed of two multilayer perceptrons that guide background consistency in deep features and suppress changes that are not of interest, such as environmental factors i.e., location and lighting. The boundary complementary information constraint is realized by adding boundary classes, which enhances the model’s attention to the position with strong uncertainty around the target of interest. The adaptive dual focus loss module uses trainable parameters to set weights for different classes, which effectively prevents the model from paying too much attention to the background class and alleviates the class imbalance. Adaptive weighted loss module weights the importance of similarity constraint and the boundary complementation information constraint. The devised approach was assessed with five mainstream advanced change detection approaches on four open-source datasets, including BCDD, LEVIR-CD, DSIFN, and S2Looking. The experimental results depicted that the proposed CDMC can achieve a state-of-the-art accuracy, convergence speed and stability. Through analyzing statistical results and visualization results, the proposed CDMC reduces the number of false positive samples and the uncertainty around the target. Additionally, the nonparametric test results of Friedman ranking also verify that CDMC is more optimal than the other five baseline models.},
	language = {en},
	urldate = {2025-05-15},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Zhang, Cheng-jie and Liu, Jian-wei},
	month = jul,
	year = {2024},
	keywords = {type:{Multi-Constraint and Hybrid}},
  series = {Engineering Applications of Artificial Intelligence},
	pages = {108163},
	file = {PDF:files/1/Zhang and Liu - 2024 - Change detection with incorporating multi-constraints and loss weights.pdf:application/pdf},
}

@article{gajowniczek_semantic_2020,
	title = {Semantic and {Generalized} {Entropy} {Loss} {Functions} for {Semi}-{Supervised} {Deep} {Learning}},
	volume = {22},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/22/3/334},
	doi = {10.3390/e22030334},
	abstract = {The increasing size of modern datasets combined with the diﬃculty of obtaining real label information (e.g., class) has made semi-supervised learning a problem of considerable practical importance in modern data analysis. Semi-supervised learning is supervised learning with additional information on the distribution of the examples or, simultaneously, an extension of unsupervised learning guided by some constraints. In this article we present a methodology that bridges between artiﬁcial neural network output vectors and logical constraints. In order to do this, we present a semantic loss function and a generalized entropy loss function (Rényi entropy) that capture how close the neural network is to satisfying the constraints on its output. Our methods are intended to be generally applicable and compatible with any feedforward neural network. Therefore, the semantic loss and generalized entropy loss are simply a regularization term that can be directly plugged into an existing loss function. We evaluate our methodology over an artiﬁcially simulated dataset and two commonly used benchmark datasets which are MNIST and Fashion-MNIST to assess the relation between the analyzed loss functions and the inﬂuence of the various input and tuning parameters on the classiﬁcation accuracy. The experimental evaluation shows that both losses eﬀectively guide the learner to achieve (near-) state-of-the-art results on semi-supervised multiclass classiﬁcation.},
	language = {en},
	number = {3},
	urldate = {2025-05-15},
	journal = {Entropy},
	author = {Gajowniczek, Krzysztof and Liang, Yitao and Friedman, Tal and Ząbkowski, Tomasz and Van Den Broeck, Guy},
	month = mar,
	year = {2020},
	keywords = {type:{Multi-Constraint and Hybrid}},
	series = {Entropy},
	pages = {334},
	file = {PDF:files/3/Gajowniczek et al. - 2020 - Semantic and Generalized Entropy Loss Functions for Semi-Supervised Deep Learning.pdf:application/pdf},
}

@misc{gonzalez-almagro_semi-supervised_2023,
	title = {Semi-supervised {Clustering} with {Two} {Types} of {Background} {Knowledge}: {Fusing} {Pairwise} {Constraints} and {Monotonicity} {Constraints}},
	shorttitle = {Semi-supervised {Clustering} with {Two} {Types} of {Background} {Knowledge}},
	url = {http://arxiv.org/abs/2302.14060},
	doi = {10.48550/arXiv.2302.14060},
	abstract = {This study addresses the problem of performing clustering in the presence of two types of background knowledge: pairwise constraints and monotonicity constraints. To achieve this, the formal framework to perform clustering under monotonicity constraints is, ﬁrstly, deﬁned, resulting in a speciﬁc distance measure. Pairwise constraints are integrated afterwards by designing an objective function which combines the proposed distance measure and a pairwise constraint-based penalty term, in order to fuse both types of information. This objective function can be optimized with an EM optimization scheme. The proposed method serves as the ﬁrst approach to the problem it addresses, as it is the ﬁrst method designed to work with the two types of background knowledge mentioned above. Our proposal is tested in a variety of benchmark datasets and in a real-world case of study.},
	language = {en},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {González-Almagro, Germán and Suárez, Juan Luis and Sánchez-Bermejo, Pablo and Cano, José-Ramón and García, Salvador},
	month = feb,
	year = {2023},
	note = {arXiv:2302.14060 [cs]},
	keywords = {type:{Multi-Constraint and Hybrid}},
	series = {Elsevier},
	file = {PDF:files/5/González-Almagro et al. - 2023 - Semi-supervised Clustering with Two Types of Background Knowledge Fusing Pairwise Constraints and M.pdf:application/pdf},
}

@article{chen_theory-guided_2021,
	title = {Theory-guided hard constraint projection ({HCP}): {A} knowledge-based data-driven scientific machine learning method},
	volume = {445},
	issn = {00219991},
	shorttitle = {Theory-guided hard constraint projection ({HCP})},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999121005192},
	doi = {10.1016/j.jcp.2021.110624},
	language = {en},
	urldate = {2025-05-15},
	journal = {Journal of Computational Physics},
	author = {Chen, Yuntian and Huang, Dou and Zhang, Dongxiao and Zeng, Junsheng and Wang, Nanzhe and Zhang, Haoran and Yan, Jinyue},
	month = nov,
	year = {2021},
	keywords = {type:{hard constraint}},
	series = {Journal of Computational Physics},
	pages = {110624},
	file = {PDF:files/6/Chen et al. - 2021 - Theory-guided hard constraint projection (HCP) A knowledge-based data-driven scientific machine lea.pdf:application/pdf},
}

@inproceedings{muralidhar_incorporating_2018,
	address = {Seattle, WA, USA},
	title = {Incorporating {Prior} {Domain} {Knowledge} into {Deep} {Neural} {Networks}},
	isbn = {978-1-5386-5035-6},
	url = {https://ieeexplore.ieee.org/document/8621955/},
	doi = {10.1109/BigData.2018.8621955},
	abstract = {In recent years, the large amount of labeled data available has also helped tend research toward using minimal domain knowledge, e.g., in deep neural network research. However, in many situations, data is limited and of poor quality. Can domain knowledge be useful in such a setting? In this paper, we propose domain adapted neural networks (DANN) to explore how domain knowledge can be integrated into model training for deep networks. In particular, we incorporate loss terms for knowledge available as monotonicity constraints and approximation constraints. We evaluate our model on both synthetic data generated using the popular Bohachevsky function and a real-world dataset for predicting oxygen solubility in water. In both situations, we ﬁnd that our DANN model outperforms its domain-agnostic counterpart yielding an overall mean performance improvement of 19.5\% with a worst- and best-case performance improvement of 4\% and 42.7\%, respectively.},
	language = {en},
	urldate = {2025-05-15},
	booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Muralidhar, Nikhil and Islam, Mohammad Raihanul and Marwah, Manish and Karpatne, Anuj and Ramakrishnan, Naren},
	month = dec,
	year = {2018},
	keywords = {type:{Multi-Constraint and Hybrid}},
	series = {IEEE},
	pages = {36--45},
	file = {PDF:files/9/Muralidhar et al. - 2018 - Incorporating Prior Domain Knowledge into Deep Neural Networks.pdf:application/pdf},
}

@article{repetto_multicriteria_2025,
	title = {Multicriteria interpretability driven deep learning},
	volume = {346},
	issn = {0254-5330, 1572-9338},
	url = {https://link.springer.com/10.1007/s10479-022-04692-6},
	doi = {10.1007/s10479-022-04692-6},
	abstract = {Deep Learning methods are well-known for their abilities, but their interpretability keeps them out of high-stakes situations. This difﬁculty is addressed by recent model-agnostic methods that provide explanations after the training process. As a result, the current guidelines’ requirement for “interpretability from the start” is not met. As a result, such methods are only useful as a sanity check after the model has been trained. In an abstract scenario, “interpretability from the start” implies imposing a set of soft constraints on the model’s behavior by infusing knowledge and eliminating any biases. By inserting knowledge into the objective function, we present a Multicriteria technique that allows us to control the feature effects on the model’s output. To accommodate for more complex effects and local lack of information, we enhance the method by integrating particular knowledge functions. As a result, a Deep Learning training process that is both interpretable and compliant with modern legislation has been developed. Our technique develops performant yet robust models capable of overcoming biases resulting from data scarcity, according to a practical empirical example based on credit risk.},
	language = {en},
	number = {2},
	urldate = {2025-05-15},
	journal = {Ann Oper Res},
	author = {Repetto, Marco},
	month = mar,
	year = {2025},
	keywords = {type:{Monotonic and Gradient-Based}},
	series = {Ann Oper Res},
	pages = {1621--1635},
	file = {PDF:files/11/Repetto - 2025 - Multicriteria interpretability driven deep learning.pdf:application/pdf},
}

@article{fajemisin_optimization_2024,
	title = {Optimization with constraint learning: {A} framework and survey},
	volume = {314},
	issn = {03772217},
	shorttitle = {Optimization with constraint learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221723003405},
	doi = {10.1016/j.ejor.2023.04.041},
	abstract = {Many real-life optimization problems frequently contain one or more constraints or objectives for which there are no explicit formulae. If however data on feasible and/or infeasible states are available, these data can be used to learn the constraints. The beneﬁts of this approach are clearly seen, however, there is a need for this process to be carried out in a structured manner. This paper, therefore, provides a framework for Optimization with Constraint Learning (OCL) which we believe will help to formalize and direct the process of learning constraints from data. This framework includes the following steps: (i) setup of the conceptual optimization model, (ii) data gathering and preprocessing, (iii) selection and training of predictive models, (iv) resolution of the optimization model, and (v) veriﬁcation and improvement of the optimization model. We then review the recent OCL literature in light of this framework and highlight current trends, as well as areas for future research.},
	language = {en},
	number = {1},
	urldate = {2025-05-15},
	journal = {European Journal of Operational Research},
	author = {Fajemisin, Adejuyigbe O. and Maragno, Donato and Den Hertog, Dick},
	month = apr,
	year = {2024},
	keywords = {type:{Survey paper, Multi-Constraint and Hybrid}},
	series = {European Journal of Operational Research},
	pages = {1--14},
	file = {PDF:files/13/Fajemisin et al. - 2024 - Optimization with constraint learning A framework and survey.pdf:application/pdf},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {00219991},
	shorttitle = {Physics-informed neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	language = {en},
	urldate = {2025-05-15},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
	month = feb,
	year = {2019},
	keywords = {type:{hard constraint}},
	series = {Journal of Computational Physics},
	pages = {686--707},
	file = {PDF:files/14/Raissi et al. - 2019 - Physics-informed neural networks A deep learning framework for solving forward and inverse problems.pdf:application/pdf},
}

@article{roychowdhury_regularizing_2021,
	title = {Regularizing deep networks with prior knowledge: {A} constraint-based approach},
	volume = {222},
	issn = {09507051},
	shorttitle = {Regularizing deep networks with prior knowledge},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121002525},
	doi = {10.1016/j.knosys.2021.106989},
	abstract = {Deep Learning architectures can develop feature representations and classification models in an integrated way during training. This joint learning process requires large networks with many parameters, and it is successful when a large amount of training data is available. Instead of making the learner develop its entire understanding of the world from scratch from the input examples, the injection of prior knowledge into the learner seems to be a principled way to reduce the amount of require training data, as the learner does not need to induce the rules from the data. This paper presents a general framework to integrate arbitrary prior knowledge into learning. The domain knowledge is provided as a collection of first-order logic (FOL) clauses, where each task to be learned corresponds to a predicate in the knowledge base. The logic statements are translated into a set of differentiable constraints, which can be integrated into the learning process to distill the knowledge into the network, or used during inference to enforce the consistency of the predictions with the prior knowledge. The experimental results have been carried out on multiple image datasets and show that the integration of the prior knowledge boosts the accuracy of several state-of-the-art deep architectures on image classification tasks.},
	language = {en},
	urldate = {2025-05-15},
	journal = {Knowledge-Based Systems},
	author = {Roychowdhury, Soumali and Diligenti, Michelangelo and Gori, Marco},
	month = jun,
	year = {2021},
	keywords = {type:{Symbolic and Logic-Based}},
	series = {Knowledge-Based Systems},
	pages = {106989},
	file = {PDF:files/17/Roychowdhury et al. - 2021 - Regularizing deep networks with prior knowledge A constraint-based approach.pdf:application/pdf},
}

@article{diligenti_semantic-based_2017,
	title = {Semantic-based regularization for learning and inference},
	volume = {244},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370215001344},
	doi = {10.1016/j.artint.2015.08.011},
	language = {en},
	urldate = {2025-05-15},
	journal = {Artificial Intelligence},
	author = {Diligenti, Michelangelo and Gori, Marco and Saccà, Claudio},
	month = mar,
	year = {2017},
	keywords = {type:{Symbolic and Logic-Based}},
	series = {Artificial Intelligence},
	pages = {143--165},
	file = {PDF:files/19/Diligenti et al. - 2017 - Semantic-based regularization for learning and inference.pdf:application/pdf},
}
