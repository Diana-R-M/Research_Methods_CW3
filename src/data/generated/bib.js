const generatedBibEntries = {
    "chen_theory-guided_2021": {
        "author": "Chen, Yuntian and Huang, Dou and Zhang, Dongxiao and Zeng, Junsheng and Wang, Nanzhe and Zhang, Haoran and Yan, Jinyue",
        "doi": "10.1016/j.jcp.2021.110624",
        "file": "PDF:files/6/Chen et al. - 2021 - Theory-guided hard constraint projection (HCP) A knowledge-based data-driven scientific machine lea.pdf:application/pdf",
        "issn": "00219991",
        "journal": "Journal of Computational Physics",
        "keywords": "type:{hard constraint}",
        "language": "en",
        "month": "nov,",
        "pages": "110624",
        "series": "Journal of Computational Physics",
        "shorttitle": "Theory-guided hard constraint projection ({HCP})",
        "title": "Theory-guided hard constraint projection ({HCP}): {A} knowledge-based data-driven scientific machine learning method",
        "type": "article",
        "url": "https://linkinghub.elsevier.com/retrieve/pii/S0021999121005192",
        "urldate": "2025-05-15",
        "volume": "445",
        "year": "2021"
    },
    "diligenti_semantic-based_2017": {
        "author": "Diligenti, Michelangelo and Gori, Marco and Sacc\u00e0, Claudio",
        "doi": "10.1016/j.artint.2015.08.011",
        "file": "PDF:files/19/Diligenti et al. - 2017 - Semantic-based regularization for learning and inference.pdf:application/pdf",
        "issn": "00043702",
        "journal": "Artificial Intelligence",
        "keywords": "type:{Symbolic and Logic-Based}",
        "language": "en",
        "month": "mar,",
        "pages": "143--165",
        "series": "Artificial Intelligence",
        "title": "Semantic-based regularization for learning and inference",
        "type": "article",
        "url": "https://linkinghub.elsevier.com/retrieve/pii/S0004370215001344",
        "urldate": "2025-05-15",
        "volume": "244",
        "year": "2017"
    },
    "fajemisin_optimization_2024": {
        "abstract": "Many real-life optimization problems frequently contain one or more constraints or objectives for which there are no explicit formulae. If however data on feasible and/or infeasible states are available, these data can be used to learn the constraints. The bene\ufb01ts of this approach are clearly seen, however, there is a need for this process to be carried out in a structured manner. This paper, therefore, provides a framework for Optimization with Constraint Learning (OCL) which we believe will help to formalize and direct the process of learning constraints from data. This framework includes the following steps: (i) setup of the conceptual optimization model, (ii) data gathering and preprocessing, (iii) selection and training of predictive models, (iv) resolution of the optimization model, and (v) veri\ufb01cation and improvement of the optimization model. We then review the recent OCL literature in light of this framework and highlight current trends, as well as areas for future research.",
        "author": "Fajemisin, Adejuyigbe O. and Maragno, Donato and Den Hertog, Dick",
        "doi": "10.1016/j.ejor.2023.04.041",
        "file": "PDF:files/13/Fajemisin et al. - 2024 - Optimization with constraint learning A framework and survey.pdf:application/pdf",
        "issn": "03772217",
        "journal": "European Journal of Operational Research",
        "keywords": "type:{Survey paper, Multi-Constraint and Hybrid}",
        "language": "en",
        "month": "apr,",
        "number": "1",
        "pages": "1--14",
        "series": "European Journal of Operational Research",
        "shorttitle": "Optimization with constraint learning",
        "title": "Optimization with constraint learning: {A} framework and survey",
        "type": "article",
        "url": "https://linkinghub.elsevier.com/retrieve/pii/S0377221723003405",
        "urldate": "2025-05-15",
        "volume": "314",
        "year": "2024"
    },
    "gajowniczek_semantic_2020": {
        "abstract": "The increasing size of modern datasets combined with the di\ufb03culty of obtaining real label information (e.g., class) has made semi-supervised learning a problem of considerable practical importance in modern data analysis. Semi-supervised learning is supervised learning with additional information on the distribution of the examples or, simultaneously, an extension of unsupervised learning guided by some constraints. In this article we present a methodology that bridges between arti\ufb01cial neural network output vectors and logical constraints. In order to do this, we present a semantic loss function and a generalized entropy loss function (R\u00e9nyi entropy) that capture how close the neural network is to satisfying the constraints on its output. Our methods are intended to be generally applicable and compatible with any feedforward neural network. Therefore, the semantic loss and generalized entropy loss are simply a regularization term that can be directly plugged into an existing loss function. We evaluate our methodology over an arti\ufb01cially simulated dataset and two commonly used benchmark datasets which are MNIST and Fashion-MNIST to assess the relation between the analyzed loss functions and the in\ufb02uence of the various input and tuning parameters on the classi\ufb01cation accuracy. The experimental evaluation shows that both losses e\ufb00ectively guide the learner to achieve (near-) state-of-the-art results on semi-supervised multiclass classi\ufb01cation.",
        "author": "Gajowniczek, Krzysztof and Liang, Yitao and Friedman, Tal and Z\u0105bkowski, Tomasz and Van Den Broeck, Guy",
        "copyright": "https://creativecommons.org/licenses/by/4.0/",
        "doi": "10.3390/e22030334",
        "file": "PDF:files/3/Gajowniczek et al. - 2020 - Semantic and Generalized Entropy Loss Functions for Semi-Supervised Deep Learning.pdf:application/pdf",
        "issn": "1099-4300",
        "journal": "Entropy",
        "keywords": "type:{Multi-Constraint and Hybrid}",
        "language": "en",
        "month": "mar,",
        "number": "3",
        "pages": "334",
        "series": "Entropy",
        "title": "Semantic and {Generalized} {Entropy} {Loss} {Functions} for {Semi}-{Supervised} {Deep} {Learning}",
        "type": "article",
        "url": "https://www.mdpi.com/1099-4300/22/3/334",
        "urldate": "2025-05-15",
        "volume": "22",
        "year": "2020"
    },
    "gonzalez-almagro_semi-supervised_2023": {
        "abstract": "This study addresses the problem of performing clustering in the presence of two types of background knowledge: pairwise constraints and monotonicity constraints. To achieve this, the formal framework to perform clustering under monotonicity constraints is, \ufb01rstly, de\ufb01ned, resulting in a speci\ufb01c distance measure. Pairwise constraints are integrated afterwards by designing an objective function which combines the proposed distance measure and a pairwise constraint-based penalty term, in order to fuse both types of information. This objective function can be optimized with an EM optimization scheme. The proposed method serves as the \ufb01rst approach to the problem it addresses, as it is the \ufb01rst method designed to work with the two types of background knowledge mentioned above. Our proposal is tested in a variety of benchmark datasets and in a real-world case of study.",
        "author": "Gonz\u00e1lez-Almagro, Germ\u00e1n and Su\u00e1rez, Juan Luis and S\u00e1nchez-Bermejo, Pablo and Cano, Jos\u00e9-Ram\u00f3n and Garc\u00eda, Salvador",
        "doi": "10.48550/arXiv.2302.14060",
        "file": "PDF:files/5/Gonz\u00e1lez-Almagro et al. - 2023 - Semi-supervised Clustering with Two Types of Background Knowledge Fusing Pairwise Constraints and M.pdf:application/pdf",
        "keywords": "type:{Multi-Constraint and Hybrid}",
        "language": "en",
        "month": "feb,",
        "note": "arXiv:2302.14060 [cs]",
        "publisher": "arXiv",
        "series": "Elsevier",
        "shorttitle": "Semi-supervised {Clustering} with {Two} {Types} of {Background} {Knowledge}",
        "title": "Semi-supervised {Clustering} with {Two} {Types} of {Background} {Knowledge}: {Fusing} {Pairwise} {Constraints} and {Monotonicity} {Constraints}",
        "type": "misc",
        "url": "http://arxiv.org/abs/2302.14060",
        "urldate": "2025-05-15",
        "year": "2023"
    },
    "muralidhar_incorporating_2018": {
        "abstract": "In recent years, the large amount of labeled data available has also helped tend research toward using minimal domain knowledge, e.g., in deep neural network research. However, in many situations, data is limited and of poor quality. Can domain knowledge be useful in such a setting? In this paper, we propose domain adapted neural networks (DANN) to explore how domain knowledge can be integrated into model training for deep networks. In particular, we incorporate loss terms for knowledge available as monotonicity constraints and approximation constraints. We evaluate our model on both synthetic data generated using the popular Bohachevsky function and a real-world dataset for predicting oxygen solubility in water. In both situations, we \ufb01nd that our DANN model outperforms its domain-agnostic counterpart yielding an overall mean performance improvement of 19.5\\% with a worst- and best-case performance improvement of 4\\% and 42.7\\%, respectively.",
        "address": "Seattle, WA, USA",
        "author": "Muralidhar, Nikhil and Islam, Mohammad Raihanul and Marwah, Manish and Karpatne, Anuj and Ramakrishnan, Naren",
        "booktitle": "2018 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})",
        "doi": "10.1109/BigData.2018.8621955",
        "file": "PDF:files/9/Muralidhar et al. - 2018 - Incorporating Prior Domain Knowledge into Deep Neural Networks.pdf:application/pdf",
        "isbn": "978-1-5386-5035-6",
        "keywords": "type:{Multi-Constraint and Hybrid}",
        "language": "en",
        "month": "dec,",
        "pages": "36--45",
        "publisher": "IEEE",
        "series": "IEEE",
        "title": "Incorporating {Prior} {Domain} {Knowledge} into {Deep} {Neural} {Networks}",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/document/8621955/",
        "urldate": "2025-05-15",
        "year": "2018"
    },
    "raissi_physics-informed_2019": {
        "author": "Raissi, M. and Perdikaris, P. and Karniadakis, G.E.",
        "doi": "10.1016/j.jcp.2018.10.045",
        "file": "PDF:files/14/Raissi et al. - 2019 - Physics-informed neural networks A deep learning framework for solving forward and inverse problems.pdf:application/pdf",
        "issn": "00219991",
        "journal": "Journal of Computational Physics",
        "keywords": "type:{hard constraint}",
        "language": "en",
        "month": "feb,",
        "pages": "686--707",
        "series": "Journal of Computational Physics",
        "shorttitle": "Physics-informed neural networks",
        "title": "Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
        "type": "article",
        "url": "https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125",
        "urldate": "2025-05-15",
        "volume": "378",
        "year": "2019"
    },
    "repetto_multicriteria_2025": {
        "abstract": "Deep Learning methods are well-known for their abilities, but their interpretability keeps them out of high-stakes situations. This dif\ufb01culty is addressed by recent model-agnostic methods that provide explanations after the training process. As a result, the current guidelines\u2019 requirement for \u201cinterpretability from the start\u201d is not met. As a result, such methods are only useful as a sanity check after the model has been trained. In an abstract scenario, \u201cinterpretability from the start\u201d implies imposing a set of soft constraints on the model\u2019s behavior by infusing knowledge and eliminating any biases. By inserting knowledge into the objective function, we present a Multicriteria technique that allows us to control the feature effects on the model\u2019s output. To accommodate for more complex effects and local lack of information, we enhance the method by integrating particular knowledge functions. As a result, a Deep Learning training process that is both interpretable and compliant with modern legislation has been developed. Our technique develops performant yet robust models capable of overcoming biases resulting from data scarcity, according to a practical empirical example based on credit risk.",
        "author": "Repetto, Marco",
        "doi": "10.1007/s10479-022-04692-6",
        "file": "PDF:files/11/Repetto - 2025 - Multicriteria interpretability driven deep learning.pdf:application/pdf",
        "issn": "0254-5330, 1572-9338",
        "journal": "Ann Oper Res",
        "keywords": "type:{Monotonic and Gradient-Based}",
        "language": "en",
        "month": "mar,",
        "number": "2",
        "pages": "1621--1635",
        "series": "Ann Oper Res",
        "title": "Multicriteria interpretability driven deep learning",
        "type": "article",
        "url": "https://link.springer.com/10.1007/s10479-022-04692-6",
        "urldate": "2025-05-15",
        "volume": "346",
        "year": "2025"
    },
    "roychowdhury_regularizing_2021": {
        "abstract": "Deep Learning architectures can develop feature representations and classification models in an integrated way during training. This joint learning process requires large networks with many parameters, and it is successful when a large amount of training data is available. Instead of making the learner develop its entire understanding of the world from scratch from the input examples, the injection of prior knowledge into the learner seems to be a principled way to reduce the amount of require training data, as the learner does not need to induce the rules from the data. This paper presents a general framework to integrate arbitrary prior knowledge into learning. The domain knowledge is provided as a collection of first-order logic (FOL) clauses, where each task to be learned corresponds to a predicate in the knowledge base. The logic statements are translated into a set of differentiable constraints, which can be integrated into the learning process to distill the knowledge into the network, or used during inference to enforce the consistency of the predictions with the prior knowledge. The experimental results have been carried out on multiple image datasets and show that the integration of the prior knowledge boosts the accuracy of several state-of-the-art deep architectures on image classification tasks.",
        "author": "Roychowdhury, Soumali and Diligenti, Michelangelo and Gori, Marco",
        "doi": "10.1016/j.knosys.2021.106989",
        "file": "PDF:files/17/Roychowdhury et al. - 2021 - Regularizing deep networks with prior knowledge A constraint-based approach.pdf:application/pdf",
        "issn": "09507051",
        "journal": "Knowledge-Based Systems",
        "keywords": "type:{Symbolic and Logic-Based}",
        "language": "en",
        "month": "jun,",
        "pages": "106989",
        "series": "Knowledge-Based Systems",
        "shorttitle": "Regularizing deep networks with prior knowledge",
        "title": "Regularizing deep networks with prior knowledge: {A} constraint-based approach",
        "type": "article",
        "url": "https://linkinghub.elsevier.com/retrieve/pii/S0950705121002525",
        "urldate": "2025-05-15",
        "volume": "222",
        "year": "2021"
    },
    "zhang_change_2024": {
        "abstract": "The past decades have seen significant progress in change detection for remote sensing images, particularly in urban change analysis and land management. The current mainstream change detection models mainly adopt the network structures of the Combination of Siamese networks and UNet. However, as the previous research mentioned, there are still several challenges to be tackled. On the one hand, A higher number of false positive samples than is the case in false negative samples, in other words, false pixel always occurs in the background region. On the other hand, speckle noise is also a serious problem to be resolved. To address these limitations, our work proposes a novel method, named the Change Detection model with incorporating Multi-Constraints and loss weights (CDMC). We introduce the similarity constraint and the boundary complementation information constraint into SUNet for the first time and design the adaptive dual focus loss module and adaptive weighted loss module. Similarity constraint is composed of two multilayer perceptrons that guide background consistency in deep features and suppress changes that are not of interest, such as environmental factors i.e., location and lighting. The boundary complementary information constraint is realized by adding boundary classes, which enhances the model\u2019s attention to the position with strong uncertainty around the target of interest. The adaptive dual focus loss module uses trainable parameters to set weights for different classes, which effectively prevents the model from paying too much attention to the background class and alleviates the class imbalance. Adaptive weighted loss module weights the importance of similarity constraint and the boundary complementation information constraint. The devised approach was assessed with five mainstream advanced change detection approaches on four open-source datasets, including BCDD, LEVIR-CD, DSIFN, and S2Looking. The experimental results depicted that the proposed CDMC can achieve a state-of-the-art accuracy, convergence speed and stability. Through analyzing statistical results and visualization results, the proposed CDMC reduces the number of false positive samples and the uncertainty around the target. Additionally, the nonparametric test results of Friedman ranking also verify that CDMC is more optimal than the other five baseline models.",
        "author": "Zhang, Cheng-jie and Liu, Jian-wei",
        "doi": "10.1016/j.engappai.2024.108163",
        "file": "PDF:files/1/Zhang and Liu - 2024 - Change detection with incorporating multi-constraints and loss weights.pdf:application/pdf",
        "issn": "09521976",
        "journal": "Engineering Applications of Artificial Intelligence",
        "keywords": "type:{Multi-Constraint and Hybrid}",
        "language": "en",
        "month": "jul,",
        "pages": "108163",
        "series": "Engineering Applications of Artificial Intelligence",
        "title": "Change detection with incorporating multi-constraints and loss weights",
        "type": "article",
        "url": "https://linkinghub.elsevier.com/retrieve/pii/S095219762400321X",
        "urldate": "2025-05-15",
        "volume": "133",
        "year": "2024"
    }
};